{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Students write text here, explaining:\n",
    " - What it means to preprocess data and what is a preprocessing pipeline\n",
    " - What are the main artefacts that can be found in the TMS-EEG signal and their causes\n",
    " - **Optional**: a personal comment or opinion on the problems posed by preprocessing, based on what you heard in class during both theoretical lessons and the first _briefing_\n",
    "\n",
    "\n",
    "\n",
    "**Ans:**\n",
    "\n",
    "Preprocessing refers to the process of separating signal from noise in acquired data, transforming raw data to allow for ease of analysis. In the context of EEG, this is the separation of scalp voltage generated by the cerebral cortex in response to stimuli (e.g., TMS pulse) from scalp voltage generated by non-cerebral sources.\n",
    "\n",
    "A preprocessing pipeline refers to the sequential transformation of data, through multiple steps. \n",
    "\n",
    "A current challenge posed by the preprocessing of TMS-EEG data is the lack of a standardized pipeline. Pipeline approaches can range from being conservationist to interventionist, varying on a case-by-case basis. Conservational approaches are more likely to preserve data as was acquired, but researchers run the risk of over-emphasising redundant data patterns; interventionist approaches tend to clean data more aggressively, but that can lead to the loss of information of interest. While the lack of a standardized pipeline may allow researchers to adapt to preprocessing different types of datasets more flexibly, this can reduce result replicability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path                            # to build a bridge between Python and the filesystem \n",
    "\n",
    "import numpy as np                                  # to perform array operations\n",
    "import matplotlib                                   # this and the following to draw plots\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Qt5Agg')                            # to make plots interactive\n",
    "\n",
    "import mne                                          # to read and manipulate EEG data\n",
    "\n",
    "from scripts import utils                           # custom functions to shorten the code in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **1. Basic Preprocessing (Assignment 1, Deadline 26/11/2025)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Students write text here, listing the operations that we categorise as \"basic preprocessing\".\n",
    "\n",
    "If you cannot remember what the steps are, look at the code in the cells below and you will recall. \n",
    "\n",
    "\n",
    "**Ans:** \n",
    "1. Loading data\n",
    "2. Inspecting data\n",
    "3. Adjusting data (dropping non-EEG channels and setting channel location)\n",
    "4. Interpolating pulse artefact\n",
    "5. High-pass filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.1. Loading and Inspecting Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional but welcome:** students write here any considerations about this step.\n",
    "\n",
    "\n",
    "**Ans:**\n",
    "\n",
    "The code below points Python to the data folder (Path(\"data\")) and instructs it to load the \"S02C1_M1\" datasets (\"subject_and_condition\" variable), identify the file of interest (S01C1_M1.vhdr), return the output on screen so the user may ensure the correct files were loaded (\"print\"), and to read it in BrainVision format (mne.io.read_raw_brainvision). The results are assigned to a variable (eeg_data) where EEG is represented as a channels x times matrix, along with accompanying metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WindowsPath('data/S02C1_M1.vhdr')] | <class 'list'> | data\\S02C1_M1.vhdr\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(\"data\")\n",
    "subject_and_condition = \"S02C1_M1\"\n",
    "\n",
    "file_of_interest = list(data_dir.glob(pattern=f\"{subject_and_condition}.vhdr\"))\n",
    "print(file_of_interest, \"|\", type(file_of_interest), \"|\", file_of_interest[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameters from data\\S02C1_M1.vhdr...\n",
      "Setting channel info structure...\n"
     ]
    }
   ],
   "source": [
    "eeg_data = mne.io.read_raw_brainvision(vhdr_fname=file_of_interest[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.2. Drop Unwated Channels**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compulsory:** students write here the functional significance of this step (that is, why we do it).\n",
    "\n",
    "\n",
    "**Ans:**\n",
    "\n",
    "The code below allows for the identification of channels not of interest to data analysis by identifying those that do not follow typical EEG channel naming conventions. \n",
    "\n",
    "The code instructs Python to inspect all channel names in the dataset (for channel_name in eeg_data.ch_names), identify whether the last character of each channel name (channel_name[-1]) ends in an integer (int), and, if not (except ValueError), whether the last character is the letter \"z\". If a channel name does not fulfill either condition, Python is instructed to store it under the \"channels_to_drop\" variable. \n",
    "\n",
    "The irrelevant and dropped channel names are then displayed on screen (print). Lastly, Python is instructed to remove the irrelevant channel data from the dataset (eeg_data.drop_channels).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels to drop: ['EOG', 'FDI']\n"
     ]
    }
   ],
   "source": [
    "channels_to_drop = []\n",
    "\n",
    "for channel_name in eeg_data.ch_names:\n",
    "    try:\n",
    "        int(channel_name[-1])\n",
    "    except ValueError:\n",
    "        if channel_name[-1] != \"z\": \n",
    "            channels_to_drop.append(channel_name)\n",
    "print(f\"Channels to drop: {channels_to_drop}\")\n",
    "\n",
    "eeg_data = eeg_data.drop_channels(ch_names=channels_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.3. Set Channel Location**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compulsory:** students write here the functional significance of this step (that is, why we do it).\n",
    "\n",
    "**Ans:**\n",
    "\n",
    "Head and sensor digitization is integral in data analysis (e.g., in source reconstruction) and visualization (e.g., in constructig topoplots). However, since the current dataset contains neither, where the EEG channels are located in relation to the subject's scalp is unkown. The current step associates each channel to a set of coordinates (a.k.a. montage) on a 2D template head.\n",
    "\n",
    "The code instructs Python to use the pre-builtin \"easycap-M1\" montage as reference to generate a montage object (mne.channels.make_standard_montage). The montage object contains the channel names, each channel's x y and z coordinates, as well as identifies the 3 fiducial points. When run (easycap_m1_mongtage.plot()), the 3D coordinates of each channel are projected onto a 2D scalp template to allow for channel position visualization. Python is also instructed to ignore channels that are not available on the easycap-M1 template (on_missing=\"ignore\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">\n",
       "    // must be `var` (not `const`) because this can get embedded multiple times on a page\n",
       "var toggleVisibility = (className) => {\n",
       "\n",
       "    const elements = document.querySelectorAll(`.${className}`);\n",
       "\n",
       "    elements.forEach(element => {\n",
       "        if (element.classList.contains(\"mne-repr-section-header\")) {\n",
       "            return  // Don't collapse the section header row\n",
       "        }\n",
       "        element.classList.toggle(\"mne-repr-collapsed\");\n",
       "    });\n",
       "\n",
       "    // trigger caret to rotate\n",
       "    var sel = `.mne-repr-section-header.${className} > th.mne-repr-section-toggle > button`;\n",
       "    const button = document.querySelector(sel);\n",
       "    button.classList.toggle(\"collapsed\");\n",
       "\n",
       "    // adjust tooltip\n",
       "    sel = `tr.mne-repr-section-header.${className}`;\n",
       "    const secHeadRow = document.querySelector(sel);\n",
       "    secHeadRow.classList.toggle(\"collapsed\");\n",
       "    secHeadRow.title = secHeadRow.title === \"Hide section\" ? \"Show section\" : \"Hide section\";\n",
       "}\n",
       "</script>\n",
       "\n",
       "<style type=\"text/css\">\n",
       "    /*\n",
       "Styles in this section apply both to the sphinx-built website docs and to notebooks\n",
       "rendered in an IDE or in Jupyter. In our web docs, styles here are complemented by\n",
       "doc/_static/styles.css and other CSS files (e.g. from the sphinx theme, sphinx-gallery,\n",
       "or bootstrap). In IDEs/Jupyter, those style files are unavailable, so only the rules in\n",
       "this file apply (plus whatever default styling the IDE applies).\n",
       "*/\n",
       ".mne-repr-table {\n",
       "    display: inline;  /* prevent using full container width */\n",
       "}\n",
       ".mne-repr-table tr.mne-repr-section-header > th {\n",
       "    padding-top: 1rem;\n",
       "    text-align: left;\n",
       "    vertical-align: middle;\n",
       "}\n",
       ".mne-repr-section-toggle > button {\n",
       "    all: unset;\n",
       "    display: block;\n",
       "    height: 1rem;\n",
       "    width: 1rem;\n",
       "}\n",
       ".mne-repr-section-toggle > button > svg {\n",
       "    height: 60%;\n",
       "}\n",
       "\n",
       "/* transition (rotation) effects on the collapser button */\n",
       ".mne-repr-section-toggle > button.collapsed > svg {\n",
       "    transition: 0.1s ease-out;\n",
       "    transform: rotate(-90deg);\n",
       "}\n",
       ".mne-repr-section-toggle > button:not(.collapsed) > svg {\n",
       "    transition: 0.1s ease-out;\n",
       "    transform: rotate(0deg);\n",
       "}\n",
       "\n",
       "/* hide collapsed table rows */\n",
       ".mne-repr-collapsed {\n",
       "    display: none;\n",
       "}\n",
       "\n",
       "\n",
       "@layer {\n",
       "    /*\n",
       "    Selectors in a `@layer` will always be lower-precedence than selectors outside the\n",
       "    layer. So even though e.g. `div.output_html` is present in the sphinx-rendered\n",
       "    website docs, the styles here won't take effect there as long as some other rule\n",
       "    somewhere in the page's CSS targets the same element.\n",
       "\n",
       "    In IDEs or Jupyter notebooks, though, the CSS files from the sphinx theme,\n",
       "    sphinx-gallery, and bootstrap are unavailable, so these styles will apply.\n",
       "\n",
       "    Notes:\n",
       "\n",
       "    - the selector `.accordion-body` is for MNE Reports\n",
       "    - the selector `.output_html` is for VSCode's notebook interface\n",
       "    - the selector `.jp-RenderedHTML` is for Jupyter notebook\n",
       "    - variables starting with `--theme-` are VSCode-specific.\n",
       "    - variables starting with `--jp-` are Jupyter styles, *some of which* are also\n",
       "      available in VSCode. Here we try the `--theme-` variable first, then fall back to\n",
       "      the `--jp-` ones.\n",
       "    */\n",
       "    .mne-repr-table {\n",
       "        --mne-toggle-color: var(--theme-foreground, var(--jp-ui-font-color1));\n",
       "        --mne-button-bg-color: var(--theme-button-background, var(--jp-info-color0, var(--jp-content-link-color)));\n",
       "        --mne-button-fg-color: var(--theme-button-foreground, var(--jp-ui-inverse-font-color0, var(--jp-editor-background)));\n",
       "        --mne-button-hover-bg-color: var(--theme-button-hover-background, var(--jp-info-color1));\n",
       "        --mne-button-radius: var(--jp-border-radius, 0.25rem);\n",
       "    }\n",
       "    /* chevron position/alignment; in VSCode it looks ok without adjusting */\n",
       "    .accordion-body .mne-repr-section-toggle > button,\n",
       "    .jp-RenderedHTML .mne-repr-section-toggle > button {\n",
       "        padding: 0 0 45% 25% !important;\n",
       "    }\n",
       "    /* chevron color; MNE Report doesn't have light/dark mode */\n",
       "    div.output_html .mne-repr-section-toggle > button > svg > path,\n",
       "    .jp-RenderedHTML .mne-repr-section-toggle > button > svg > path {\n",
       "        fill: var(--mne-toggle-color);\n",
       "    }\n",
       "    .accordion-body .mne-ch-names-btn,\n",
       "    div.output_html .mne-ch-names-btn,\n",
       "    .jp-RenderedHTML .mne-ch-names-btn {\n",
       "        -webkit-border-radius: var(--mne-button-radius);\n",
       "        -moz-border-radius: var(--mne-button-radius);\n",
       "        border-radius: var(--mne-button-radius);\n",
       "        border: none;\n",
       "        background-image: none;\n",
       "        background-color: var(--mne-button-bg-color);\n",
       "        color: var(--mne-button-fg-color);\n",
       "        font-size: inherit;\n",
       "        min-width: 1.5rem;\n",
       "        padding: 0.25rem;\n",
       "        text-align: center;\n",
       "        text-decoration: none;\n",
       "    }\n",
       "    .accordion-body .mne-ch-names-btn:hover,\n",
       "    div.output_html .mne.ch-names-btn:hover,\n",
       "    .jp-RenderedHTML .mne-ch-names-btn:hover {\n",
       "        background-color: var(--mne-button-hover-bg-color);\n",
       "        text-decoration: underline;\n",
       "    }\n",
       "    .accordion-body .mne-ch-names-btn:focus-visible,\n",
       "    div.output_html .mne-ch-names-btn:focus-visible,\n",
       "    .jp-RenderedHTML .mne-ch-names-btn:focus-visible {\n",
       "        outline: 0.1875rem solid var(--mne-button-bg-color) !important;\n",
       "        outline-offset: 0.1875rem !important;\n",
       "    }\n",
       "}\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "<table class=\"table mne-repr-table\">\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<tr class=\"mne-repr-section-header general-8efe7e2d-5996-42fa-8830-825529339a27\"\n",
       "     title=\"Hide section\" \n",
       "    onclick=\"toggleVisibility('general-8efe7e2d-5996-42fa-8830-825529339a27')\">\n",
       "    <th class=\"mne-repr-section-toggle\">\n",
       "        <button >\n",
       "            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 512 512\"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d=\"M233.4 406.6c12.5 12.5 32.8 12.5 45.3 0l192-192c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L256 338.7 86.6 169.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l192 192z\"/></svg>\n",
       "        </button>\n",
       "    </th>\n",
       "    <th colspan=\"2\">\n",
       "        <strong>General</strong>\n",
       "    </th>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr class=\"repr-element general-8efe7e2d-5996-42fa-8830-825529339a27 \">\n",
       "    <td class=\"mne-repr-section-toggle\"></td>\n",
       "    <td>Filename(s)</td>\n",
       "    <td>\n",
       "        \n",
       "        S02C1_M1.eeg\n",
       "        \n",
       "        \n",
       "    </td>\n",
       "</tr>\n",
       "\n",
       "<tr class=\"repr-element general-8efe7e2d-5996-42fa-8830-825529339a27 \">\n",
       "    <td class=\"mne-repr-section-toggle\"></td>\n",
       "    <td>MNE object type</td>\n",
       "    <td>RawBrainVision</td>\n",
       "</tr>\n",
       "<tr class=\"repr-element general-8efe7e2d-5996-42fa-8830-825529339a27 \">\n",
       "    <td class=\"mne-repr-section-toggle\"></td>\n",
       "    <td>Measurement date</td>\n",
       "    \n",
       "    <td>2008-01-09 at 11:35:22 UTC</td>\n",
       "    \n",
       "</tr>\n",
       "<tr class=\"repr-element general-8efe7e2d-5996-42fa-8830-825529339a27 \">\n",
       "    <td class=\"mne-repr-section-toggle\"></td>\n",
       "    <td>Participant</td>\n",
       "    \n",
       "    <td>Unknown</td>\n",
       "    \n",
       "</tr>\n",
       "<tr class=\"repr-element general-8efe7e2d-5996-42fa-8830-825529339a27 \">\n",
       "    <td class=\"mne-repr-section-toggle\"></td>\n",
       "    <td>Experimenter</td>\n",
       "    \n",
       "    <td>Unknown</td>\n",
       "    \n",
       "</tr>\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<tr class=\"mne-repr-section-header acquisition-bcba24d4-f957-4218-8a80-f4cfab35d9c4\"\n",
       "     title=\"Hide section\" \n",
       "    onclick=\"toggleVisibility('acquisition-bcba24d4-f957-4218-8a80-f4cfab35d9c4')\">\n",
       "    <th class=\"mne-repr-section-toggle\">\n",
       "        <button >\n",
       "            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 512 512\"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d=\"M233.4 406.6c12.5 12.5 32.8 12.5 45.3 0l192-192c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L256 338.7 86.6 169.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l192 192z\"/></svg>\n",
       "        </button>\n",
       "    </th>\n",
       "    <th colspan=\"2\">\n",
       "        <strong>Acquisition</strong>\n",
       "    </th>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr class=\"repr-element acquisition-bcba24d4-f957-4218-8a80-f4cfab35d9c4 \">\n",
       "    <td class=\"mne-repr-section-toggle\"></td>\n",
       "    <td>Duration</td>\n",
       "    <td>00:11:02 (HH:MM:SS)</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<tr class=\"repr-element acquisition-bcba24d4-f957-4218-8a80-f4cfab35d9c4 \">\n",
       "    <td class=\"mne-repr-section-toggle\"></td>\n",
       "    <td>Sampling frequency</td>\n",
       "    <td>5000.00 Hz</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr class=\"repr-element acquisition-bcba24d4-f957-4218-8a80-f4cfab35d9c4 \">\n",
       "    <td class=\"mne-repr-section-toggle\"></td>\n",
       "    <td>Time points</td>\n",
       "    <td>3,307,600</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<tr class=\"mne-repr-section-header channels-addf99f3-e2a9-4b06-8e02-5939aa692d14\"\n",
       "     title=\"Hide section\" \n",
       "    onclick=\"toggleVisibility('channels-addf99f3-e2a9-4b06-8e02-5939aa692d14')\">\n",
       "    <th class=\"mne-repr-section-toggle\">\n",
       "        <button >\n",
       "            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 512 512\"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d=\"M233.4 406.6c12.5 12.5 32.8 12.5 45.3 0l192-192c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L256 338.7 86.6 169.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l192 192z\"/></svg>\n",
       "        </button>\n",
       "    </th>\n",
       "    <th colspan=\"2\">\n",
       "        <strong>Channels</strong>\n",
       "    </th>\n",
       "</tr>\n",
       "\n",
       "\n",
       "    \n",
       "<tr class=\"repr-element channels-addf99f3-e2a9-4b06-8e02-5939aa692d14 \">\n",
       "    <td class=\"mne-repr-section-toggle\"></td>\n",
       "    <td>EEG</td>\n",
       "    <td>\n",
       "        <button class=\"mne-ch-names-btn sd-sphinx-override sd-btn sd-btn-info sd-text-wrap sd-shadow-sm\" onclick=\"alert('Good EEG:\\n\\nFp1, Fpz, Fp2, AF7, AF3, AF4, AF8, F9, F7, F5, F3, F1, Fz, F2, F4, F6, F8, F10, FT9, FT7, FC5, FC3, FC1, FCz, FC2, FC4, FC6, FT8, FT10, T7, C5, C3, C1, Cz, C2, C4, C6, T8, TP9, TP7, CP5, CP3, CP1, CPz, CP2, CP4, CP6, TP8, P9, P7, P5, P3, P1, Pz, P2, P4, P6, P8, P10, PO9, PO7, P03, POz, PO4, PO8, PO10, O1, Oz, O2, Iz')\" title=\"(Click to open in popup)&#13;&#13;Fp1, Fpz, Fp2, AF7, AF3, AF4, AF8, F9, F7, F5, F3, F1, Fz, F2, F4, F6, F8, F10, FT9, FT7, FC5, FC3, FC1, FCz, FC2, FC4, FC6, FT8, FT10, T7, C5, C3, C1, Cz, C2, C4, C6, T8, TP9, TP7, CP5, CP3, CP1, CPz, CP2, CP4, CP6, TP8, P9, P7, P5, P3, P1, Pz, P2, P4, P6, P8, P10, PO9, PO7, P03, POz, PO4, PO8, PO10, O1, Oz, O2, Iz\">\n",
       "            70\n",
       "        </button>\n",
       "\n",
       "        \n",
       "    </td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr class=\"repr-element channels-addf99f3-e2a9-4b06-8e02-5939aa692d14 \">\n",
       "    <td class=\"mne-repr-section-toggle\"></td>\n",
       "    <td>Head & sensor digitization</td>\n",
       "    \n",
       "    <td>72 points</td>\n",
       "    \n",
       "</tr>\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<tr class=\"mne-repr-section-header filters-26676d08-a964-4652-96ad-e49231a5e3fb\"\n",
       "     title=\"Hide section\" \n",
       "    onclick=\"toggleVisibility('filters-26676d08-a964-4652-96ad-e49231a5e3fb')\">\n",
       "    <th class=\"mne-repr-section-toggle\">\n",
       "        <button >\n",
       "            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 512 512\"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d=\"M233.4 406.6c12.5 12.5 32.8 12.5 45.3 0l192-192c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L256 338.7 86.6 169.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l192 192z\"/></svg>\n",
       "        </button>\n",
       "    </th>\n",
       "    <th colspan=\"2\">\n",
       "        <strong>Filters</strong>\n",
       "    </th>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr class=\"repr-element filters-26676d08-a964-4652-96ad-e49231a5e3fb \">\n",
       "    <td class=\"mne-repr-section-toggle\"></td>\n",
       "    <td>Highpass</td>\n",
       "    <td>0.02 Hz</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr class=\"repr-element filters-26676d08-a964-4652-96ad-e49231a5e3fb \">\n",
       "    <td class=\"mne-repr-section-toggle\"></td>\n",
       "    <td>Lowpass</td>\n",
       "    <td>1000.00 Hz</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "</table>"
      ],
      "text/plain": [
       "<RawBrainVision | S02C1_M1.eeg, 70 x 3307600 (661.5 s), ~86 KiB, data not loaded>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easycap_m1_montage = mne.channels.make_standard_montage(kind=\"easycap-M1\")\n",
    "easycap_m1_montage.plot()\n",
    "eeg_data.set_montage(montage=easycap_m1_montage,\n",
    "                     on_missing=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compulsory:** here, students describe and comment the plot generated by the code below. In particular, I would like to read:\n",
    "\n",
    "- What each row represents\n",
    "- What are the $x$ and $y$ of each row, and their units of measurement\n",
    "- A description of the artefacts you see, and their likely causes\n",
    "\n",
    "\n",
    "**Ans:**\n",
    "\n",
    "Each row represent the electrical signal acquired from each EEG electrode channel.\n",
    "\n",
    "$x$ = Time, in seconds; $y$ = EEG electrode name (and thus the general location on scalp)\n",
    "\n",
    "Artefacts:\n",
    "- High frequency signals (e.g. channels FC3, C3, C1): Likely generated by environmental noise\n",
    "- Large fluctuations at event markers: TMS pulse introduces noise to data\n",
    "- Large, quick fluctuations most prominent at Fp channels, shrinking as channels move increasingly further from the eyes (e.g., timepoint 123s): Artifacts from eye blinks\n",
    "- Large fluctuations across all channels at specific timepoints (e.g., timepoint 27.7s): Muscle noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using qt as 2D backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mne_qt_browser._pg_figure.MNEQtBrowser at 0x20349d83770>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_data.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compulsory:** here, students define event markers and explain why they are important for EEG data analysis\n",
    "\n",
    "**Optional:** any comments that go beyond a mere definition are welcome. For example, this could be comments about the structure of event arrays and what it tells us about the nature of events\n",
    "\n",
    "\n",
    "**Ans:**\n",
    "\n",
    "Event markers provide information on the point at which an event (e.g., TMS pulse) occured. This includes its ordinal number, type, name, time of occurence, duration, and the number of channels involved. Event markers serve as a reference point in which researchers can create epochs when generating TMS-evoked potentials.\n",
    "\n",
    "The code instructs Python to extract marker information from the EEG data structure (mne.events_from_annotations(raw=eeg_data)) and associate each name to a numerical format that is readable by the program (events_dict). An array is formed, which each row (200 total) corresponding to an event's time of occurence, affected channels, and event type. It also instructs Python to extract data from the second event onwards ([2:]) as these events are likely irrelevant, such as test pulses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['New Segment/', 'Stimulus/S 54', 'Stimulus/S255']\n"
     ]
    }
   ],
   "source": [
    "events_from_annotations, events_dict = mne.events_from_annotations(raw=eeg_data)\n",
    "events_from_annotations = events_from_annotations[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.4. Interpolate the Pulse Artefact**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compulsory:** students write here the functional significance of this step (that is, why we do it).\n",
    "\n",
    "\n",
    "**Ans:**\n",
    "\n",
    "This step creates small time windoes (epochs) around the TMS pulses, averages them across the 200 trials, and plots the resulting TEPs for each channel.\n",
    "\n",
    "The code instructs Python to create an epoch around each TMS pulse (mne.Epochs), as extracted from the raw eeg_data file. The time window begins 1.1s before TMS pulse application (tmin=-1.1) and ends 0.5s after the pulse (tmax=0.5). This is repeated for each event (events=events_from_annotations) of the raw eeg data file, and results are stored in the variable \"pre_interpolation_epochs_tep\". For each channel, the epochs are averaged across trials to create a TEP (pre_interpolation_epochs.average())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Channels marked as bad:\n",
      "none\n"
     ]
    }
   ],
   "source": [
    "pre_interpolation_epochs = mne.Epochs(raw=eeg_data,\n",
    "                                      events=events_from_annotations,\n",
    "                                      tmin=-1.1,\n",
    "                                      tmax=0.5,\n",
    "                                      baseline=None)\n",
    "pre_interpolation_epochs_tep = pre_interpolation_epochs.average()\n",
    "pre_interpolation_epochs_tep.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file data/post_2_5_interpolation_eeg.fif...\n",
      "    Range : 0 ... 3307599 =      0.000 ...   661.520 secs\n",
      "Ready.\n",
      "Reading 0 ... 3307599  =      0.000 ...   661.520 secs...\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    }
   ],
   "source": [
    "post_interpolation_eeg = mne.io.read_raw(fname=\"data/post_2_5_interpolation_eeg.fif\",\n",
    "                                         preload=True)\n",
    "post_interpolation_epochs = mne.Epochs(raw=post_interpolation_eeg,\n",
    "                                       events=events_from_annotations,\n",
    "                                       tmin=-1.1,\n",
    "                                       tmax=0.5,\n",
    "                                       baseline=None)\n",
    "post_interpolation_tep = post_interpolation_epochs.average()\n",
    "post_interpolation_tep.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.5. High-Pass Filtering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compulsory:** students write here the functional significance of this step (that is, why we do it) and potential caveats (that is, one-two things that can happen if you do this step unproperly).\n",
    "\n",
    "\n",
    "**Ans:**\n",
    "\n",
    "This step sets up a high-pass filter to supress extremely low frequencies within the signal, potentially caused by artefactual sources such as sweating.\n",
    "\n",
    "The code applies a high-pass filter (mne.filter.filter_data) to attenuate signals from the data lower than 0.1Hz (1_freq=0.1). No low-pass filter is applied (h_freq=None). The filtered NumPy data is then converted into a raw object (mne.io.RawArray). The new data is epoched again for all channels (mne.Epochs) with the same time window as before (from 1.1s before a TMS pulse to 0.5s after), then averaged to generate the post-filtered TEPs (post_filtering_epochs.average()).\n",
    "\n",
    "Some potential caveats when applying filters include the accidental suppression of low-oscillatory signals of interest (e.g., alpha rhythm) when an inapporpriate filter threshold is applied. Excessive use of temporal filters may also lead to the distortion of signal amplitude and latency or the introduction of artificial peaks within the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up high-pass filter at 0.1 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth highpass zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 8 (effective, after forward-backward)\n",
      "- Cutoff at 0.10 Hz: -6.02 dB\n",
      "\n",
      "Creating RawArray with float64 data, n_channels=70, n_times=3307600\n",
      "    Range : 0 ... 3307599 =      0.000 ...   661.520 secs\n",
      "Ready.\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    }
   ],
   "source": [
    "post_filtering_eeg = mne.filter.filter_data(data=post_interpolation_eeg.get_data(),\n",
    "                                            sfreq=post_interpolation_eeg.info[\"sfreq\"],\n",
    "                                            l_freq=0.1,\n",
    "                                            h_freq=None,\n",
    "                                            method=\"iir\",\n",
    "                                            iir_params=None,\n",
    "                                            copy=True,\n",
    "                                            phase=\"zero\")\n",
    "post_filtering_eeg = mne.io.RawArray(data=post_filtering_eeg,\n",
    "                                     info=post_interpolation_eeg.info)\n",
    "post_filtering_epochs = mne.Epochs(raw=post_filtering_eeg,\n",
    "                                   events=events_from_annotations,\n",
    "                                   tmin=-1.1,\n",
    "                                   tmax=0.5,\n",
    "                                   baseline=None)\n",
    "post_filtering_tep = post_filtering_epochs.average()\n",
    "post_filtering_tep.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Independent Component Analysis (ICA) (Assignment 2, Deadline 05/12/2025)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1. Rationale**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compulsory:** students write here the functional significance of this step (that is, why we do it) and expand on its rationale, devoting particular attention to discussing the independence assumption and why its application to brain data is debatable.\n",
    "\n",
    "\n",
    "**Ans:**\n",
    "\n",
    "An Independent Component Analysis (ICA) is a mathematical procedure that breaks down, or decomposes, a signal into a set of factors. These factors must: \n",
    "1. Be linearly independent of each other (i.e., no factor can be expressed mathematically by another factor within the signal) and,\n",
    "2. After deconstruction, be reconstructable into a singular signal.\n",
    "\n",
    "In EEG preprocessing, ICAs isolate artefacts from cortical sources in a dataset. By breaking down a signal into its individual factors, researchers can identify factors most likely generated through non-neurocognitive activty (e.g., eye blinks, eye movements, or muscle activity), remove these factors, and reassemble the remaining factors to construct a cleaner signal with a lower signal-to-noise ratio for analysis.\n",
    "\n",
    "That said, ICA is a blind source-separation technique that does not account for the nature of the sources it decomposes. In the context of EEG preprocessing, this means that ICAs operate under the assumption that factors making up an EEG signal are independent from one another, further implying an assumption that neural activity in different regions of the brain do not interact with one another. This is inaccurate, given the high level of connectivity different neural systems share with one another. Additionally, ICAs assume that EEG sources remain spatially stationary throughout the period of signal recording. This does not account for the possibility that brain activity can spread across the scalp over time, such as during sleep or seizures. \n",
    "\n",
    "The incompatability between ICA's independence assumption and the brain's high level of interconnectivity means it is possible that components extracted through ICA may contain multiple neural sources that happened to have shared synchronous activity during data recording. Signals arising from the same brain phenomenon that spreads across space might also be split into separate components.  \n",
    "\n",
    "Ultimately, components extracted through ICA are purely mathematical and may not be necessarily meaningful.\n",
    "\n",
    "\n",
    "\n",
    "That said, ICAs are used extensively in the removal of artefacts -- eye blinks in particular. This is because eye blinks are relatively independent of neurocognitive activity as they hold little cognitive significance and have a stereotypical pattern and amplitude that are easily distinguishable from brain signals. ICAs may also be used to remove mucle artefacts, but this practise varies from lab to lab and should be used with caution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2. Fitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA to data using 70 channels (please be patient, this may take a while)\n",
      "Using data from preloaded Raw for 200 events and 8001 original time points ...\n",
      "Selecting by non-zero PCA components: 70 components\n",
      "Using data from preloaded Raw for 200 events and 8001 original time points ...\n",
      "Fitting ICA took 375.9s.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"table mne-repr-table\">\n",
       "    <tr>\n",
       "        <th>Method</th>\n",
       "        <td>fastica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Fit parameters</th>\n",
       "        <td>algorithm=parallel<br />fun=logcosh<br />fun_args=None<br />max_iter=1000<br /></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Fit</th>\n",
       "        <td>86 iterations on epochs (1600200 samples)</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <th>ICA components</th>\n",
       "        <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Available PCA components</th>\n",
       "        <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Channel types</th>\n",
       "        <td>eeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>ICA components marked for exclusion</th>\n",
       "        <td>&mdash;</td>\n",
       "    </tr>\n",
       "    \n",
       "</table>"
      ],
      "text/plain": [
       "<ICA | epochs decomposition, method: fastica (fit in 86 iterations on 1600200 samples), 70 ICA components (70 PCA components available), channel types: eeg, no sources marked for exclusion>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ica = mne.preprocessing.ICA(random_state=0)\n",
    "ica.fit(post_filtering_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.3. Components Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compulsory:** students select components whose most likely source are eye movements and justify their choices here, including a picture of the selected component(s).\n",
    "\n",
    "To help you tell components apart, you can refer to [this](https://labeling.ucsd.edu/tutorial/labels) tutorial by the developers of ICLabel: an algorithm to automatically classify independent components that came out of the Swartz Center for Computational Neuroscience, University of California San Diego ([Pion-Tonachini et al., 2019](https://www.sciencedirect.com/science/article/pii/S1053811919304185)).\n",
    "\n",
    "To include a picture of the selected component(s), just take a screenshot, save it somewhere and change the path below (`files/sample-ic.png`) with the path to your screenshot. \n",
    "\n",
    "Selecting no components is OK but the choice must be justified in writing.\n",
    "\n",
    "\n",
    "**Ans:**\n",
    "\n",
    "ICA001 is most likely the component whose sources are eye blinks or eye movements. This is because on its scalp topography, the component visibly affects the region around the eyes, and the power spectrum graph shows small and few peaks. When inspecting the component over time, ICA001 shows large, periodic, and steretypical deflections. The component fluctuations also correspond to the periodic, large fluctuations (likely caused by eye blinks) recorded by the Fp1 channel in the raw data. \n",
    "\n",
    "ICA021 might also be a product of eye movement, possibly from saccades or horizontal eye movement. This is because the component affects the region around the eyes, is dipolar, and the power specturm graph displays small and few peaks.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"C:\\Users\\Todd\\OneDrive\\Documents\\Masters\\Brain Stimulation and Multimodal Electrophysiological Recording\\brainstim-multimodal-main\\ICA001.png\" width=\"1000\"/>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"C:\\Users\\Todd\\OneDrive\\Documents\\Masters\\Brain Stimulation and Multimodal Electrophysiological Recording\\brainstim-multimodal-main\\ICA001 Time.png\" width=\"1000\"/>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"C:\\Users\\Todd\\OneDrive\\Documents\\Masters\\Brain Stimulation and Multimodal Electrophysiological Recording\\brainstim-multimodal-main\\ICA021.png\" width=\"1000\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MNEFigure size 975x1170 with 30 Axes>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ica.plot_components(inst=post_filtering_epochs, picks=range(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data from preloaded Raw for 200 events and 8001 original time points ...\n",
      "Not setting metadata\n",
      "200 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mne_qt_browser._pg_figure.MNEQtBrowser at 0x20329482330>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ica.plot_sources(inst=post_filtering_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_to_reject = utils.select_items(item_type=\"ica\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data from preloaded Raw for 200 events and 8001 original time points ...\n",
      "Applying ICA to Epochs instance\n",
      "    Transforming to ICA space (70 components)\n",
      "    Zeroing out 1 ICA component\n",
      "    Projecting back using 70 PCA components\n"
     ]
    }
   ],
   "source": [
    "ica.exclude = components_to_reject\n",
    "ica.apply(post_filtering_epochs.load_data())\n",
    "post_ica_epochs = post_filtering_epochs\n",
    "post_ica_tep = post_ica_epochs.average()\n",
    "post_ica_tep.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Manual Artifact Rejection (Assignment 3, Deadline 12/12/2025)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1. Rationale**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compulsory:** students write here the functional significance of this step (that is, why we do it) and expand on the criteria to follow for its execution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2. Execution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_ica_epochs.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Computing & Assessing a TEP (Assignment 4, Deadline 19/12/2025)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.1. Rationale**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compulsory:** students write here the functional significance of this step (that is, why we do it) and expand on the criteria to follow for its execution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.2. Execution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional:** Student can write here any comment or consideration that goes beyond the minimum required, for example:\n",
    "- \"I think filtering is `{good/bad}` because...\"\n",
    "- \"I think these data were particularly `{dirty/clean}` because...\"\n",
    "- Anything else you might feel like saying about the preprocessing you have done\n",
    "\n",
    "Do not be afraid to make such comments: there is no right or wrong and they do not count for the vote. At most, a great comment could make you a _cum laude_ candidate, but I really just want to see if you have a personal opinion on the subject."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
